# ğŸ§  Next Word Prediction using LSTM-Based Neural Networks

This project implements a **Next Word Prediction** model using a **Long Short-Term Memory (LSTM)** neural network.  
It learns to predict the next word in a sequence based on previous context, enabling applications such as predictive typing and text completion.

---

## ğŸš€ Overview

This project demonstrates how **LSTM models** can learn long-term dependencies in natural language data.  
The model is trained on an English text corpus and evaluated using **accuracy, loss**, and **perplexity** metrics.

Key highlights:
- Built using **Python, TensorFlow, and Keras**
- Trained in **Google Colab** with GPU acceleration
- Token-based dataset for efficient training
- Visualized training performance (accuracy & loss)
- Predicts contextually relevant next words

---

## ğŸ“ Repository Contents

```
next-word-prediction-lstm/
â”‚
â”œâ”€â”€ word_predictor.py                # Main training & prediction script
â”œâ”€â”€ dataset.tokens                   # Tokenized English text corpus (Git LFS)
â”œâ”€â”€ report.pdf                       # IEEE-format project report
â”œâ”€â”€ presentation.pdf                 # Presentation slides
â”œâ”€â”€ .gitattributes                   # Git LFS configuration
â””â”€â”€ README.md                        # Project documentation
```

---

## ğŸ§  Model Architecture

| Layer | Description |
|-------|--------------|
| **Embedding** | Converts tokens into dense 100D vectors |
| **LSTM (128 units)** | Captures sequential dependencies |
| **Dense Softmax** | Outputs probability distribution for next word |

**Optimizer:** RMSProp  
**Loss:** Sparse Categorical Crossentropy  
**Batch Size:** 128  
**Epochs:** 5  

---

## ğŸ“Š Training Results

| Metric | Description |
|---------|--------------|
| **Training Accuracy** | Increases steadily per epoch |
| **Validation Accuracy** | Follows training closely |
| **Loss** | Decreases smoothly, minimal overfitting |
| **Perplexity** | Confirms effective language modeling |

---

### ğŸ“ˆ Example Graphs

Add your plots here:

- `accuracy_plot.png`
- `loss_plot.png`
- `perplexity_plot.png`

---

## ğŸ’¬ Sample Predictions

| Input | Predicted Next Words |
|--------|----------------------|
| `the quick brown fox` | `jumps`, `runs`, `leaps` |
| `deep learning models are` | `powerful`, `complex`, `effective` |

---

## ğŸ’» Run the Project in Google Colab

You can run the full workflow in Colab.

1. Open a new Colab notebook  
2. Upload:
   - `word_predictor.py`
   - `dataset.tokens`
3. Paste and run this setup:

```python
!pip install tensorflow numpy matplotlib
from google.colab import files
uploaded = files.upload()
!python word_predictor.py
```

ğŸ‘‰ **Open in Colab:**  
[![Open In Colab]([https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abinairv/next-word-prediction-lstm/blob/main/colab_notebook.ipynb](https://colab.research.google.com/drive/1PeiOxja3ESl_KANscnQZoBs7W3ztCbw5?usp=sharing))

---

## âš™ï¸ Dependencies

Install locally:
```bash
pip install tensorflow numpy matplotlib
```

---

## ğŸ§® Evaluation Metric â€” Perplexity

Perplexity measures prediction uncertainty:

\[
Perplexity = e^{H(p)}
\]

Lower values = better next-word prediction.

---

## ğŸ“¦ Output Files

| File | Purpose |
|------|----------|
| `next_word_model_tokens.h5` | Trained model weights |
| `tokenizer_tokens.pickle` | Vocabulary tokenizer |
| `accuracy_plot.png` | Accuracy curve |
| `loss_plot.png` | Loss curve |
| `perplexity_plot.png` | Perplexity chart |

---

## ğŸ« Author Information

**Author:** Abhinay R V  
**Department:** Computer Science  
**Institution:** [Your University Name]  
**Email:** abhinayyy.rv@gmail.com  

---

## ğŸ™ Acknowledgments

- **Google Colab** â€” GPU compute resources  
- **TensorFlow/Keras** â€” Deep learning framework  
- **Faculty Supervisor** â€” Academic guidance  
- **Open English Text Corpora** â€” Dataset sources  

---

## ğŸ§¾ License

Licensed under the **MIT License** â€” free to use, modify, and distribute with attribution.

---

## â­ Final Thoughts

This project highlights how **LSTM networks** remain effective for sequence modeling and next-word prediction, even with moderate datasets and limited computational resources.

> â€œPredicting the next word isnâ€™t guessing â€” itâ€™s understanding context.â€

---

ğŸ“ **GitHub Repository:** [https://github.com/abinairv/next-word-prediction-lstm](https://github.com/abinairv/next-word-prediction-lstm)  
ğŸ•’ **Last Updated:** October 2025
